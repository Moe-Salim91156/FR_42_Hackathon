{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11851383,"sourceType":"datasetVersion","datasetId":7446879}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## TOC\n- [Imports](#Imports)\n- [Preperations](#Preperations)\n- [Training](#Training)\n- [Visualization](#Visualization)\n- [Finishing](#Finishing)","metadata":{}},{"cell_type":"markdown","source":"## Imports\n> In this section we will import the required libraries which are:  \n> `Json`: to parse json file.  \n> `Pandas`: to load the json into dataframe.  \n> `FastAI`: The library which will be used to train our model.","metadata":{}},{"cell_type":"markdown","source":"## Preperations\n> In this section we will open and parse the json file which includes the entries to train our model on then load it to dataframe then prepare a DataBlock which is high level api to tell FastAI how to deal with our data which will be splitted into training and validation data.","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\nfrom fastai.vision.all import *","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:04:27.664243Z","iopub.execute_input":"2025-05-18T08:04:27.664781Z","iopub.status.idle":"2025-05-18T08:04:40.878261Z","shell.execute_reply.started":"2025-05-18T08:04:27.664756Z","shell.execute_reply":"2025-05-18T08:04:40.877600Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_root ='/kaggle/input/0x2ahack-data'\n\nwith open(dataset_root + '/' + 'cleanResult.json') as f:\n    data = json.load(f)","metadata":{"_uuid":"de3530c3-d5c5-492a-900e-59d8e552c35d","_cell_guid":"917cf59d-5e0d-4cce-8ad4-e0255123f998","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-18T08:04:40.879368Z","iopub.execute_input":"2025-05-18T08:04:40.879758Z","iopub.status.idle":"2025-05-18T08:04:40.893698Z","shell.execute_reply.started":"2025-05-18T08:04:40.879739Z","shell.execute_reply":"2025-05-18T08:04:40.893114Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> But before we insert the entire entries to FastAI we need to further clean the data by using verify_image function to make sure its a valid image which FastAI can deal with.","metadata":{}},{"cell_type":"code","source":"verifiedData = []\nfor x in data:\n    res = verify_image(x['image_path'])\n    if res == True:\n        verifiedData.append(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:07:35.570026Z","iopub.execute_input":"2025-05-18T08:07:35.570870Z","iopub.status.idle":"2025-05-18T08:07:42.633602Z","shell.execute_reply.started":"2025-05-18T08:07:35.570838Z","shell.execute_reply":"2025-05-18T08:07:42.632894Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> Then we insert the data:","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(verifiedData)\ndblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_x=ColReader('image_path'),\n    get_y=ColReader('is_student'),\n    splitter=RandomSplitter(valid_pct=0.2, seed=42), \n    item_tfms=Resize(128),\n    batch_tfms=aug_transforms(mult=2)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:44:14.536763Z","iopub.execute_input":"2025-05-18T08:44:14.537181Z","iopub.status.idle":"2025-05-18T08:44:14.545386Z","shell.execute_reply.started":"2025-05-18T08:44:14.537154Z","shell.execute_reply":"2025-05-18T08:44:14.544553Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> Now we load our dataframe into FastAI which will be splitted into batches for more efficent access in model training, and using visions_leaner we load the model resnet34 which is used for image classification with the data provided","metadata":{}},{"cell_type":"code","source":"dls = dblock.dataloaders(df, bs=32)\ndls.show_batch(max_n=6, nrows=2, unique=False)\nlearner = vision_learner(dls,models.resnet34, pretrained=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:44:18.165218Z","iopub.execute_input":"2025-05-18T08:44:18.165509Z","iopub.status.idle":"2025-05-18T08:44:19.790803Z","shell.execute_reply.started":"2025-05-18T08:44:18.165487Z","shell.execute_reply":"2025-05-18T08:44:19.790032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"learner.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:44:40.448132Z","iopub.execute_input":"2025-05-18T08:44:40.448436Z","iopub.status.idle":"2025-05-18T08:44:40.992343Z","shell.execute_reply.started":"2025-05-18T08:44:40.448405Z","shell.execute_reply":"2025-05-18T08:44:40.991582Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> Now the resnet34 model works by stepping on batches of data and generate parameters from it, now how far the model steps each iteration affects the quality of our final model thats why we need to choose it carefully, choosing the right value requires a lot of testing thankfully FastAI has function called lr_find which finds the optimal length of steps which has the least accuracy loss.","metadata":{}},{"cell_type":"code","source":"learner.lr_find()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:44:46.988094Z","iopub.execute_input":"2025-05-18T08:44:46.988911Z","iopub.status.idle":"2025-05-18T08:45:26.111823Z","shell.execute_reply.started":"2025-05-18T08:44:46.988879Z","shell.execute_reply":"2025-05-18T08:45:26.110862Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training\n> Now we train the pretrained resnet34 model on our data to identify the features in our images and generate paramaters from them which can be used to make predictions.","metadata":{}},{"cell_type":"code","source":"learner.fine_tune(4, base_lr=0.0030199517495930195)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:45:42.911218Z","iopub.execute_input":"2025-05-18T08:45:42.911523Z","iopub.status.idle":"2025-05-18T08:48:56.641994Z","shell.execute_reply.started":"2025-05-18T08:45:42.911496Z","shell.execute_reply":"2025-05-18T08:48:56.641138Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> After the training is done we show few of the validation data results with our trained model.","metadata":{}},{"cell_type":"code","source":"learner.show_results()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T09:20:43.236435Z","iopub.execute_input":"2025-05-18T09:20:43.237354Z","iopub.status.idle":"2025-05-18T09:20:45.333333Z","shell.execute_reply.started":"2025-05-18T09:20:43.237313Z","shell.execute_reply":"2025-05-18T09:20:45.332036Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualization\n> Now to identify the accuracy of our trained model we plot a visualization called confusion matrix which shows the count of correct and incorrect predictions on our validation set.","metadata":{}},{"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learner)\ninterp.plot_confusion_matrix()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:50:42.000328Z","iopub.execute_input":"2025-05-18T08:50:42.001044Z","iopub.status.idle":"2025-05-18T08:50:55.149484Z","shell.execute_reply.started":"2025-05-18T08:50:42.001018Z","shell.execute_reply":"2025-05-18T08:50:55.148764Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"interp.plot_top_losses(6,figsize = (25,5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T09:22:03.235297Z","iopub.execute_input":"2025-05-18T09:22:03.235608Z","iopub.status.idle":"2025-05-18T09:22:04.845811Z","shell.execute_reply.started":"2025-05-18T09:22:03.235572Z","shell.execute_reply":"2025-05-18T09:22:04.844819Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Finishing\n> Last step is to export the model into pickel data strcture which includes all the nessesary data to make predictions offline.","metadata":{}},{"cell_type":"code","source":"learner.export('trained_model.pkl')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:49:15.277022Z","iopub.execute_input":"2025-05-18T08:49:15.277345Z","iopub.status.idle":"2025-05-18T08:49:15.600947Z","shell.execute_reply.started":"2025-05-18T08:49:15.277314Z","shell.execute_reply":"2025-05-18T08:49:15.600130Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}